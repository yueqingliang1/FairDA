{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp4_optim1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pandas.core.frame import DataFrame\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from fairlearn.metrics import MetricFrame\n",
    "import ipdb\n",
    "import argparse\n",
    "\n",
    "seed = 8\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 320\n",
    "random_state = seed\n",
    "# shuffle_dataset = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- data ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- source -------------\n",
    "\n",
    "sens_attr = 'sex'\n",
    "predict_attr = 'is_higher_than_50k'\n",
    "\n",
    "src = fetch_openml(data_id=1590)\n",
    "\n",
    "src.data[predict_attr] = src.target.map({'<=50K':0, '>50K':1})\n",
    "header = list(src.data.columns)\n",
    "src.data.dropna(inplace=True)\n",
    "src.data[predict_attr] = src.data[predict_attr].astype(int)\n",
    "\n",
    "src.data = src.data[src.data['native-country']!='United-States']\n",
    "y_true_src = src.data[predict_attr].values\n",
    "\n",
    "sensitive_attr_src = src.data[sens_attr]\n",
    "A1_true = pd.get_dummies(sensitive_attr_src)\n",
    "A1_true = A1_true.drop(['Male'], axis=1)\n",
    "A1_true = A1_true['Female'].values\n",
    "\n",
    "header.remove(sens_attr)\n",
    "header.remove(predict_attr)\n",
    "\n",
    "src.data = src.data[header]\n",
    "\n",
    "X_src = pd.get_dummies(src.data)\n",
    "X_src = X_src.sort_index(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# ------------- target -------------\n",
    "\n",
    "tgt = fetch_openml(data_id=1590)\n",
    "\n",
    "header = list(tgt.data.columns)\n",
    "\n",
    "tgt.data[predict_attr] = tgt.target.map({'<=50K':0, '>50K':1})\n",
    "tgt.data.dropna(inplace=True)\n",
    "tgt.data[predict_attr] = tgt.data[predict_attr].astype(int)\n",
    "tgt.data = tgt.data[tgt.data['native-country']=='United-States']\n",
    "y_true_tgt = tgt.data[predict_attr].values\n",
    "\n",
    "sensitive_attr_tgt = tgt.data[sens_attr]\n",
    "\n",
    "header.remove(sens_attr)\n",
    "\n",
    "tgt.data = tgt.data[header]\n",
    "\n",
    "X_tgt = pd.get_dummies(tgt.data)\n",
    "X_tgt = X_tgt.sort_index(axis=1)\n",
    "\n",
    "n_classes = y_true_tgt.max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- preprocess ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# source training samples: 2358\n",
      "# source batches: 7\n",
      "# target training samples: 20646\n",
      "# target batches: 64\n"
     ]
    }
   ],
   "source": [
    "class PandasDataSet(TensorDataset):\n",
    "\n",
    "    def __init__(self, *dataframes):\n",
    "        tensors = (self._df_to_tensor(df) for df in dataframes)\n",
    "        super(PandasDataSet, self).__init__(*tensors)\n",
    "\n",
    "    def _df_to_tensor(self, df):\n",
    "        if isinstance(df, np.ndarray):\n",
    "            return torch.from_numpy(df).float()\n",
    "        return torch.from_numpy(df.values).float()\n",
    "\n",
    "# ------------- source -------------\n",
    "# if domain=='source': train/test = 0.6:0.4\n",
    "train_ratio = 0.6\n",
    "test_ratio = 0.4\n",
    "\n",
    "indict_src = np.arange(sensitive_attr_src.shape[0])\n",
    "(X_train_src, X_test_src, y_train_src, y_test_src, A1_train, A1_test, ind_train_src, ind_test_src) = train_test_split(X_src, y_true_src, A1_true, indict_src, test_size=test_ratio, stratify=y_true_src, random_state=random_state)\n",
    "\n",
    "# processed_X_train_src = X_train_src\n",
    "\n",
    "# standardize the data\n",
    "scaler_src = StandardScaler().fit(X_train_src)\n",
    "X_train_src = scaler_src.transform(X_train_src)\n",
    "X_test_src = scaler_src.transform(X_test_src)\n",
    "\n",
    "train_data_src = PandasDataSet(X_train_src, y_train_src, A1_train, ind_train_src)\n",
    "test_data_src = PandasDataSet(X_test_src, y_test_src, A1_test, ind_test_src)\n",
    "\n",
    "src_train_loader = DataLoader(train_data_src, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "print('# source training samples:', len(train_data_src))\n",
    "print('# source batches:', len(src_train_loader))\n",
    "\n",
    "# ------------- target -------------\n",
    "# else: domain=='target': train/valid/test set = 0.5:0.25:0.25\n",
    "train_ratio = 0.5\n",
    "test_ratio = 0.25\n",
    "valid_ratio = 0.25\n",
    "\n",
    "# split into train/test set\n",
    "indict_tgt = np.arange(sensitive_attr_tgt.shape[0])\n",
    "(X_train_tgt, X_test_tgt, y_train_tgt, y_test_tgt, ind_train_tgt, ind_test_tgt) = train_test_split(X_tgt, y_true_tgt, indict_tgt, test_size=test_ratio, stratify=y_true_tgt, random_state=random_state)\n",
    "\n",
    "# split training set into train/validation set\n",
    "(X_train_tgt, X_valid_tgt, y_train_tgt, y_valid_tgt, ind_train_tgt, ind_valid_tgt) = train_test_split(X_train_tgt, y_train_tgt, ind_train_tgt, test_size=valid_ratio/(train_ratio+valid_ratio), stratify=y_train_tgt, random_state=random_state)\n",
    "\n",
    "# processed_X_train_tgt = X_train_tgt\n",
    "\n",
    "# standardize the data\n",
    "scaler_tgt = StandardScaler().fit(X_train_tgt)\n",
    "X_train_tgt = scaler_tgt.transform(X_train_tgt)\n",
    "X_valid_tgt = scaler_tgt.transform(X_valid_tgt)\n",
    "X_test_tgt = scaler_tgt.transform(X_test_tgt)\n",
    "\n",
    "train_data_tgt = PandasDataSet(X_train_tgt, y_train_tgt, ind_train_tgt)\n",
    "test_data_tgt = PandasDataSet(X_test_tgt, y_test_tgt, ind_test_tgt)\n",
    "valid_data_tgt = PandasDataSet(X_valid_tgt, y_valid_tgt, ind_valid_tgt)\n",
    "\n",
    "tgt_train_loader = DataLoader(train_data_tgt, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "print('# target training samples:', len(train_data_tgt))\n",
    "print('# target batches:', len(tgt_train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapper(nn.Module):\n",
    "    \"\"\"\n",
    "        Mapping feature dimensions of src and tgt domain into the same.\n",
    "    \"\"\"\n",
    "    def __init__(self, ori_dims, n_features=16):\n",
    "        super().__init__()\n",
    "        self.map = nn.Sequential(\n",
    "            nn.Linear(ori_dims, n_features),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        m = self.map(x).squeeze()\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Hidden Embedding Layer h\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features=16, n_hidden=32, p_dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Sequential(\n",
    "            nn.Linear(n_features, n_hidden*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(n_hidden*2, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "#             nn.Linear(n_hidden, n_class),\n",
    "        )\n",
    "\n",
    "    def forward(self, m):\n",
    "        emb = self.emb(m)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "        Classification head\n",
    "    \"\"\"\n",
    "    def __init__(self, n_hidden=32, n_class=2):\n",
    "        super().__init__()\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_class),\n",
    "        )\n",
    "    def forward(self, emb):\n",
    "        cls = self.cls(emb)\n",
    "        return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg()\n",
    "\n",
    "        return output, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "        Discrimination head\n",
    "    \"\"\"\n",
    "    def __init__(self, n_hidden=32, n_adv=2):\n",
    "        super().__init__()\n",
    "        self.adv = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_adv),\n",
    "        )\n",
    "    def forward(self, emb):\n",
    "        reversed_input = ReverseLayerF.apply(emb)\n",
    "        adv = self.adv(reversed_input)\n",
    "        return adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_dims_src = X_src.shape[1]\n",
    "ori_dims_tgt = X_tgt.shape[1]\n",
    "lr = 0.0001\n",
    "\n",
    "# n_features = X.shape[1]\n",
    "\n",
    "M1 = Mapper(ori_dims=ori_dims_src).to(DEVICE)\n",
    "M2 = Mapper(ori_dims=ori_dims_tgt).to(DEVICE)\n",
    "H1 = Encoder().to(DEVICE)\n",
    "H2 = Encoder().to(DEVICE)\n",
    "FA = Classifier().to(DEVICE)\n",
    "L2 = Classifier().to(DEVICE)\n",
    "D1 = Discriminator().to(DEVICE)\n",
    "D2 = Discriminator().to(DEVICE)\n",
    "\n",
    "L_params = list(M1.parameters()) + list(M2.parameters()) + list(H1.parameters()) + list(FA.parameters()) + list(D1.parameters())\n",
    "R_params = list(H2.parameters()) + list(L2.parameters()) + list(D2.parameters())\n",
    "\n",
    "L_optim = optim.Adam(L_params, lr = lr)\n",
    "R_optim = optim.Adam(R_params, lr = lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FairDA_transfer_train(M1, M2, H1, FA, D1, src_train_loader, L_optim, criterion):\n",
    "\n",
    "    for batch_idx, (src, tgt) in enumerate(zip(src_train_loader, tgt_train_loader)):\n",
    "        X1_train, _, A1_train, _ = src\n",
    "        X2_train, Y2_train, _ = tgt\n",
    "        \n",
    "        X1 = X1_train.to(torch.float32).to(DEVICE)\n",
    "        X2 = X2_train.to(torch.float32).to(DEVICE)\n",
    "        A1 = A1_train.to(torch.float32).to(DEVICE)\n",
    "        \n",
    "        L_optim.zero_grad()\n",
    "        \n",
    "        m1 = M1(X1)\n",
    "        m2 = M2(X2)\n",
    "        m12 = torch.cat([m1, m2], dim=0)\n",
    "        h1 = H1(m12)\n",
    "        \n",
    "        \n",
    "        # ---------------------------\n",
    "        #   Train: FA, H1, M1, M2\n",
    "        # ---------------------------\n",
    "        A1_hat = FA(h1[:batch_size])\n",
    "        loss_FA = criterion(A1_hat, A1.long())\n",
    "        \n",
    "        # --------------\n",
    "        #    Train D1\n",
    "        # --------------\n",
    "        D1_hat = D1(h1)\n",
    "        \n",
    "        D_src = torch.zeros(X1.shape[0]).to(DEVICE)\n",
    "        D_tgt = torch.ones(X2.shape[0]).to(DEVICE)\n",
    "        D_labels = torch.cat([D_src, D_tgt], dim=0).long()\n",
    "        \n",
    "        loss_D1 = criterion(D1_hat, D_labels)\n",
    "        \n",
    "        # -------------------\n",
    "        #    Transfer loss\n",
    "        # -------------------\n",
    "        loss_left = loss_FA + loss_D1\n",
    "        loss_left.backward()\n",
    "        \n",
    "        L_optim.step()\n",
    "        \n",
    "\n",
    "    return M1, M2, H1, FA, D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FairDA_debias_train(H2, L2, D2, FA, H1, M2, tgt_train_loader, R_optim, criterion, D2_weights):\n",
    "    for batch_idx, (X2_train, Y2_train, _) in enumerate(tgt_train_loader):\n",
    "        \n",
    "        X2 = X2_train.to(torch.float32).to(DEVICE)\n",
    "        Y2 = Y2_train.to(torch.float32).to(DEVICE)\n",
    "        \n",
    "        R_optim.zero_grad()\n",
    "\n",
    "        h2 = H2(M2(X2))\n",
    "\n",
    "        # ---------------------------\n",
    "        #   Train: H2, L2\n",
    "        # ---------------------------\n",
    "        Y2_hat = L2(h2)\n",
    "        loss_L2 = criterion(Y2_hat, Y2.long())\n",
    "\n",
    "        # --------------\n",
    "        #    Train D2\n",
    "        # --------------\n",
    "        A2_hat = D2(h2)\n",
    "        A2_ground = FA(H1(M2(X2))).detach()\n",
    "        A2_ground = A2_ground.argmax(dim=1)\n",
    "        loss_D2 = criterion(A2_hat, A2_ground)\n",
    "        \n",
    "        # --------------------\n",
    "        #    Debiasing loss\n",
    "        # --------------------\n",
    "        loss_right = loss_L2 + D2_weights * loss_D2\n",
    "        loss_right.backward()\n",
    "\n",
    "        R_optim.step()\n",
    "        \n",
    "        \n",
    "    return H2, L2, D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_metric(sens_ind, y2_pred, y2_true):\n",
    "\n",
    "    group_dp = []\n",
    "    group_equal_odds = []\n",
    "    sens_data = sensitive_attr_tgt.iloc[sens_ind]\n",
    "\n",
    "\n",
    "    for sens_value in set(sens_data):\n",
    "        y_sense_pred = y2_pred[(sens_data==sens_value).values]\n",
    "        y_sense_test = y2_true[(sens_data==sens_value).values]\n",
    "        sens_dp = []\n",
    "        sens_eo = []\n",
    "\n",
    "        for label in set(y2_true):\n",
    "            if label>0:\n",
    "                sens_dp_label = (y_sense_pred==label).sum()/y_sense_pred.shape[0]\n",
    "                sens_eo_label = (y_sense_pred[y_sense_test==label]==label).sum()/(y_sense_test==label).sum()\n",
    "\n",
    "                sens_dp.append(sens_dp_label)\n",
    "                sens_eo.append(sens_eo_label)\n",
    "\n",
    "        group_dp.append(sens_dp)\n",
    "        group_equal_odds.append(sens_eo)\n",
    "\n",
    "    group_dp = np.array(group_dp)\n",
    "    group_eo = np.array(group_equal_odds)\n",
    "\n",
    "    dp_diff = np.mean(np.absolute(group_dp-np.mean(group_dp, axis=0, keepdims=True)))\n",
    "    eo_diff = np.mean(np.absolute(group_equal_odds-np.mean(group_equal_odds, axis=0, keepdims=True)))\n",
    "\n",
    "    return group_dp, group_eo, dp_diff, eo_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "                 Pretraining on source finished                   \n",
      "                       Best epoch: 0118                         \n",
      "                 Best ACC on pretraining: 0.8581                  \n",
      "==================================================================\n",
      "Group DP:\n",
      "[[0.26681066]\n",
      " [0.03641208]]\n",
      "Group EO:\n",
      "[[0.5306394 ]\n",
      " [0.21111111]]\n",
      "Group DP:\n",
      "[[0.262491  ]\n",
      " [0.03641208]]\n",
      "Group EO:\n",
      "[[0.52886325]\n",
      " [0.21111111]]\n",
      "Group DP:\n",
      "[[0.26580274]\n",
      " [0.04055654]]\n",
      "Group EO:\n",
      "[[0.53507996]\n",
      " [0.2361111 ]]\n",
      "Group DP:\n",
      "[[0.2650828 ]\n",
      " [0.04174067]]\n",
      "Group EO:\n",
      "[[0.53419185]\n",
      " [0.24444444]]\n",
      "Group DP:\n",
      "[[0.26148307]\n",
      " [0.04114861]]\n",
      "Group EO:\n",
      "[[0.5315275 ]\n",
      " [0.23888889]]\n",
      "Group DP:\n",
      "[[0.26191506]\n",
      " [0.04233274]]\n",
      "Group EO:\n",
      "[[0.5328597 ]\n",
      " [0.24444444]]\n",
      "Group DP:\n",
      "[[0.26018718]\n",
      " [0.04292481]]\n",
      "Group EO:\n",
      "[[0.5315275]\n",
      " [0.25     ]]\n",
      "Group DP:\n",
      "[[0.262347  ]\n",
      " [0.04410894]]\n",
      "Group EO:\n",
      "[[0.5346359 ]\n",
      " [0.25833333]]\n",
      "Group DP:\n",
      "[[0.25788337]\n",
      " [0.04410894]]\n",
      "Group EO:\n",
      "[[0.5306394 ]\n",
      " [0.25555557]]\n",
      "Group DP:\n",
      "[[0.25572354]\n",
      " [0.04499704]]\n",
      "Group EO:\n",
      "[[0.5293073]\n",
      " [0.2638889]]\n",
      "Group DP:\n",
      "[[0.25946724]\n",
      " [0.04618117]]\n",
      "Group EO:\n",
      "[[0.5346359 ]\n",
      " [0.27222222]]\n",
      "Group DP:\n",
      "[[0.25831532]\n",
      " [0.04795737]]\n",
      "Group EO:\n",
      "[[0.5346359 ]\n",
      " [0.28333333]]\n",
      "Group DP:\n",
      "[[0.2573074 ]\n",
      " [0.04795737]]\n",
      "Group EO:\n",
      "[[0.5337478 ]\n",
      " [0.28333333]]\n",
      "Group DP:\n",
      "[[0.25845933]\n",
      " [0.04854944]]\n",
      "Group EO:\n",
      "[[0.535968  ]\n",
      " [0.28611112]]\n",
      "Group DP:\n",
      "[[0.2587473 ]\n",
      " [0.04943754]]\n",
      "Group EO:\n",
      "[[0.53641206]\n",
      " [0.29166666]]\n",
      "Group DP:\n",
      "[[0.25701943]\n",
      " [0.04943754]]\n",
      "Group EO:\n",
      "[[0.53507996]\n",
      " [0.29722223]]\n",
      "Group DP:\n",
      "[[0.2550036 ]\n",
      " [0.04973357]]\n",
      "Group EO:\n",
      "[[0.5319716 ]\n",
      " [0.30277777]]\n",
      "Group DP:\n",
      "[[0.2548596 ]\n",
      " [0.04943754]]\n",
      "Group EO:\n",
      "[[0.53108346]\n",
      " [0.30555555]]\n",
      "Group DP:\n",
      "[[0.25716344]\n",
      " [0.05062167]]\n",
      "Group EO:\n",
      "[[0.53507996]\n",
      " [0.31388888]]\n",
      "Group DP:\n",
      "[[0.25557956]\n",
      " [0.0509177 ]]\n",
      "Group EO:\n",
      "[[0.53419185]\n",
      " [0.31666666]]\n",
      "Group DP:\n",
      "[[0.25514758]\n",
      " [0.05032564]]\n",
      "Group EO:\n",
      "[[0.53330374]\n",
      " [0.31388888]]\n",
      "Group DP:\n",
      "[[0.2525558 ]\n",
      " [0.05062167]]\n",
      "Group EO:\n",
      "[[0.5306394 ]\n",
      " [0.31388888]]\n",
      "Group DP:\n",
      "[[0.25687546]\n",
      " [0.05239787]]\n",
      "Group EO:\n",
      "[[0.535968  ]\n",
      " [0.32222223]]\n",
      "Group DP:\n",
      "[[0.25385168]\n",
      " [0.05210184]]\n",
      "Group EO:\n",
      "[[0.53330374]\n",
      " [0.31944445]]\n",
      "Group DP:\n",
      "[[0.26205903]\n",
      " [0.05328597]]\n",
      "Group EO:\n",
      "[[0.5452931]\n",
      " [0.325    ]]\n",
      "Group DP:\n",
      "[[0.25831532]\n",
      " [0.0544701 ]]\n",
      "Group EO:\n",
      "[[0.54085255]\n",
      " [0.33333334]]\n",
      "Group DP:\n",
      "[[0.2512599]\n",
      " [0.053582 ]]\n",
      "Group EO:\n",
      "[[0.5324156 ]\n",
      " [0.33333334]]\n",
      "Group DP:\n",
      "[[0.2512599]\n",
      " [0.053582 ]]\n",
      "Group EO:\n",
      "[[0.5328597 ]\n",
      " [0.33333334]]\n",
      "Group DP:\n",
      "[[0.2512599]\n",
      " [0.0544701]]\n",
      "Group EO:\n",
      "[[0.53419185]\n",
      " [0.3361111 ]]\n",
      "Group DP:\n",
      "[[0.2525558 ]\n",
      " [0.05476613]]\n",
      "Group EO:\n",
      "[[0.5368561 ]\n",
      " [0.33888888]]\n",
      "Group DP:\n",
      "[[0.25212383]\n",
      " [0.0562463 ]]\n",
      "Group EO:\n",
      "[[0.5377442]\n",
      " [0.3472222]]\n",
      "Group DP:\n",
      "[[0.25644347]\n",
      " [0.05831853]]\n",
      "Group EO:\n",
      "[[0.54795736]\n",
      " [0.35833332]]\n",
      "Group DP:\n",
      "[[0.25183585]\n",
      " [0.05743043]]\n",
      "Group EO:\n",
      "[[0.54085255]\n",
      " [0.35277778]]\n",
      "Group DP:\n",
      "[[0.24910007]\n",
      " [0.0571344 ]]\n",
      "Group EO:\n",
      "[[0.53507996]\n",
      " [0.35555556]]\n"
     ]
    }
   ],
   "source": [
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epoch_pre = 150\n",
    "n_epoch_tgt = 150\n",
    "\n",
    "D1_weights = 0.01\n",
    "D2_weights = 0.01\n",
    "\n",
    "best_acc_pre = 0\n",
    "best_epoch_pre = -1\n",
    "\n",
    "# while best_epoch_pre < 50:\n",
    "    \n",
    "for epoch in range(1, n_epoch_pre):\n",
    "\n",
    "    M1 = M1.train()\n",
    "    M2 = M2.train()\n",
    "    H1 = H1.train()\n",
    "    FA = FA.train()\n",
    "    D1 = D1.train()\n",
    "\n",
    "\n",
    "    M1, M2, H1, FA, D1 = FairDA_transfer_train(M1, M2, H1, FA, D1, \n",
    "                                               src_train_loader, \n",
    "                                               L_optim, criterion=ce)\n",
    "    M1 = M1.eval()\n",
    "    M2 = M2.eval()\n",
    "    H1 = H1.eval()\n",
    "    FA = FA.eval()\n",
    "    D1 = D1.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pre_a1_test = FA(H1(M1(test_data_src.tensors[0].to(DEVICE))))\n",
    "        a1_pred = pre_a1_test.argmax(dim=1)\n",
    "        acc_a1 = accuracy_score(A1_test, a1_pred.cpu())\n",
    "\n",
    "        if acc_a1 > best_acc_pre:\n",
    "            best_acc_pre = acc_a1\n",
    "            best_epoch_pre = epoch\n",
    "            torch.save(FA,\"saved_models/FA.pt\")\n",
    "            torch.save(H1,\"saved_models/H1.pt\")\n",
    "            torch.save(M2,\"saved_models/M2.pt\")\n",
    "\n",
    "    \n",
    "print('==================================================================')\n",
    "print('                 Pretraining on source finished                   ')\n",
    "print('                       Best epoch: {:04d}                         '.format(best_epoch_pre+1))\n",
    "print('                 Best ACC on pretraining: {:.4f}                  '.format(best_acc_pre))\n",
    "print('==================================================================')\n",
    "\n",
    "\n",
    "FA = torch.load(\"saved_models/FA.pt\")\n",
    "H1 = torch.load(\"saved_models/H1.pt\")\n",
    "M2 = torch.load(\"saved_models/M2.pt\")\n",
    "FA.eval()\n",
    "H1.eval()\n",
    "M2.eval()\n",
    "\n",
    "best_result = {}\n",
    "best_epoch = -1\n",
    "best_fair = 100\n",
    "acc_thrsh = 0.8\n",
    "F1_thrsh = 0.5\n",
    "\n",
    "# while best_epoch < 50:\n",
    "    \n",
    "for epoch in range(1, n_epoch_tgt):\n",
    "\n",
    "    H2 = H2.train()\n",
    "    L2 = L2.train()\n",
    "    D2 = D2.train()\n",
    "\n",
    "    H2, L2, D2 = FairDA_debias_train(H2, L2, D2, FA, H1, M2, \n",
    "                                     tgt_train_loader, \n",
    "                                     R_optim, criterion=ce, \n",
    "                                     D2_weights = D2_weights)\n",
    "    H2 = H2.eval()\n",
    "    L2 = L2.eval()\n",
    "    D2 = D2.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pre_y2_val = L2(H2(M2(valid_data_tgt.tensors[0].to(DEVICE))))\n",
    "        y2_pre_val = pre_y2_val.argmax(dim=1).cpu()\n",
    "        acc_y2_val = accuracy_score(y_valid_tgt, y2_pre_val)\n",
    "        f1_y2_val = f1_score(y_valid_tgt, y2_pre_val)\n",
    "        _, _, dp_diff_val, eo_diff_val = fair_metric(ind_valid_tgt, y2_pre_val, y_valid_tgt)\n",
    "\n",
    "        pre_y2_test = L2(H2(M2(test_data_tgt.tensors[0].to(DEVICE))))\n",
    "        y2_pred = pre_y2_test.argmax(dim=1).cpu()\n",
    "        acc_y2 = accuracy_score(y_test_tgt, y2_pred)\n",
    "        f1_y2 = f1_score(y_test_tgt, y2_pred)\n",
    "        group_dp, group_eo, dp_diff, eo_diff = fair_metric(ind_test_tgt, y2_pred, y_test_tgt)\n",
    "\n",
    "#             print('************ Test result: accuracy_Y2: {:.4f}, F1_Y2: {:.4f}       ***********'.format(acc_y2, f1_y2))\n",
    "#             print('================================================================================')\n",
    "\n",
    "\n",
    "\n",
    "        if acc_y2_val > acc_thrsh and f1_y2_val > F1_thrsh:\n",
    "            if best_fair > dp_diff_val + eo_diff_val:\n",
    "                best_fair = dp_diff_val + eo_diff_val\n",
    "                best_result['Epoch'] = epoch\n",
    "                best_result['ACC'] = acc_y2\n",
    "                best_result['F1'] = f1_y2\n",
    "                best_result['DP'] = dp_diff\n",
    "                best_result['EO'] = eo_diff\n",
    "                print('Group DP:')\n",
    "                print(group_dp)\n",
    "                print('Group EO:')\n",
    "                print(group_eo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================ Performace on Test ============================\n",
      "************         best epoch: 91.0000                       ***********\n",
      "************         best ACC: 0.8189, best F1: 0.5879        ***********\n",
      "************         best DP: 0.0960, best EO: 0.0898        ***********\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('============================ Performace on Test ============================')\n",
    "if len(best_result) > 0:\n",
    "    \n",
    "    print('************         best epoch: {:.4f}                       ***********'.format(best_result['Epoch']))\n",
    "    print('************         best ACC: {:.4f}, best F1: {:.4f}        ***********'.format(best_result['ACC'], best_result['F1']))\n",
    "    print('************         best DP: {:.4f}, best EO: {:.4f}        ***********'.format(best_result['DP'], best_result['EO']))\n",
    "    print('================================================================================')\n",
    "\n",
    "else:\n",
    "    print('Please set smaller ACC and F1 thresholds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
